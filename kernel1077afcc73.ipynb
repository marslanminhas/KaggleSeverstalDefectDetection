{"cells":[{"metadata":{"_uuid":"7126da8a-2003-4b02-a624-e1460e2764d0","_cell_guid":"137ecee0-59be-4455-92cf-b40628a0e46f","trusted":true},"cell_type":"code","source":"import argparse\nimport json\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom math import ceil\nfrom glob import glob\nfrom PIL import Image\nimport pandas as pd\nimport random\nfrom PIL import Image\nimport cv2\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport PIL\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom itertools import chain\nimport logging\nimport numpy as np\nimport math\nimport torch\nimport os\nimport sys\nimport zipfile\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn as nn\nfrom scipy import ndimage\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\ntry:\n    from urllib import urlretrieve\nexcept ImportError:\n    from urllib.request import urlretrieve\n\n\nclass BaseDataSet(Dataset):\n    def __init__(self, root, split, mean, std, base_size=None, augment=True, val=False,\n                 crop_size=321, scale=True, flip=True, rotate=False, blur=False, return_id=False):\n        self.root = root\n        self.split = split\n        self.mean = mean\n        self.std = std\n        self.augment = augment\n        self.crop_size = crop_size\n        if self.augment:\n            self.base_size = base_size\n            self.scale = scale\n            self.flip = flip\n            self.rotate = rotate\n            self.blur = blur\n        self.val = val\n        self.files = []\n        self._set_files()\n        self.to_tensor = transforms.ToTensor()\n        self.normalize = transforms.Normalize(mean, std)\n        self.return_id = return_id\n\n        cv2.setNumThreads(0)\n\n    def _set_files(self):\n        raise NotImplementedError\n\n    def _load_data(self, index):\n        raise NotImplementedError\n\n    def _val_augmentation(self, image, label):\n        if self.crop_size:\n            h, w = label.shape\n            # Scale the smaller side to crop size\n            if h < w:\n                h, w = (self.crop_size, int(self.crop_size * w / h))\n            else:\n                h, w = (int(self.crop_size * h / w), self.crop_size)\n\n            image = cv2.resize(image, (w, h), interpolation=cv2.INTER_LINEAR)\n            label = Image.fromarray(label).resize((w, h), resample=Image.NEAREST)\n            label = np.asarray(label, dtype=np.int32)\n\n            # Center Crop\n            h, w = label.shape\n            start_h = (h - self.crop_size) // 2\n            start_w = (w - self.crop_size) // 2\n            end_h = start_h + self.crop_size\n            end_w = start_w + self.crop_size\n            image = image[start_h:end_h, start_w:end_w]\n            label = label[start_h:end_h, start_w:end_w]\n        return image, label\n\n    def _augmentation(self, image, label):\n        h, w, _ = image.shape\n        # Scaling, we set the bigger to base size, and the smaller\n        # one is rescaled to maintain the same ratio, if we don't have any obj in the image, re-do the processing\n        if self.base_size:\n            if self.scale:\n                longside = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))\n            else:\n                longside = self.base_size\n            h, w = (longside, int(1.0 * longside * w / h + 0.5)) if h > w else (\n                int(1.0 * longside * h / w + 0.5), longside)\n            image = cv2.resize(image, (w, h), interpolation=cv2.INTER_LINEAR)\n            label = cv2.resize(label, (w, h), interpolation=cv2.INTER_NEAREST)\n\n        image_new, label_new = image, label\n        h, w, _ = image_new.shape\n        # Rotate the image with an angle between -10 and 10\n        if self.rotate:\n            angle = random.randint(-10, 10)\n            center = (w / 2, h / 2)\n            rot_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n            image_new = cv2.warpAffine(image_new, rot_matrix, (w, h),\n                                       flags=cv2.INTER_LINEAR)  # , borderMode=cv2.BORDER_REFLECT)\n            label_new = cv2.warpAffine(label_new, rot_matrix, (w, h),\n                                       flags=cv2.INTER_NEAREST)  # ,  borderMode=cv2.BORDER_REFLECT)\n\n        # Padding to return the correct crop size\n        if self.crop_size:\n            pad_h = max(self.crop_size - h, 0)\n            pad_w = max(self.crop_size - w, 0)\n            pad_kwargs = {\n                \"top\": 0,\n                \"bottom\": pad_h,\n                \"left\": 0,\n                \"right\": pad_w,\n                \"borderType\": cv2.BORDER_CONSTANT, }\n            if pad_h > 0 or pad_w > 0:\n                image_new = cv2.copyMakeBorder(image_new, value=0, **pad_kwargs)\n                label_new = cv2.copyMakeBorder(label_new, value=0, **pad_kwargs)\n\n            # Cropping\n            h, w, _ = image_new.shape\n            start_h = random.randint(0, h - self.crop_size)\n            start_w = random.randint(0, w - self.crop_size)\n            end_h = start_h + self.crop_size\n            end_w = start_w + self.crop_size\n            image_new = image_new[start_h:end_h, start_w:end_w]\n            label_new = label_new[start_h:end_h, start_w:end_w]\n\n            image = image_new\n            label = label_new\n\n        # Random H flip\n        if self.flip:\n            if random.random() > 0.5:\n                image = np.fliplr(image).copy()\n                label = np.fliplr(label).copy()\n\n        # Gaussian Blud (sigma between 0 and 1.5)\n        if self.blur:\n            sigma = random.random()\n            ksize = int(3.3 * sigma)\n            ksize = ksize + 1 if ksize % 2 == 0 else ksize\n            image = cv2.GaussianBlur(image, (ksize, ksize), sigmaX=sigma, sigmaY=sigma,\n                                     borderType=cv2.BORDER_REFLECT_101)\n        return image, label\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        image, label, image_id = self._load_data(index)\n        if self.val:\n            image, label = self._val_augmentation(image, label)\n        elif self.augment:\n            image, label = self._augmentation(image, label)\n        # label = torch.from_numpy(np.array(label, dtype=np.int32)).long()\n        label = torch.from_numpy(np.array(label, dtype=np.int8)).long()\n        image = Image.fromarray(np.uint8(image))\n        if self.return_id:\n            return self.normalize(self.to_tensor(image)), label, image_id\n        return self.normalize(self.to_tensor(image)), label\n\n    def __repr__(self):\n        fmt_str = \"Dataset: \" + self.__class__.__name__ + \"\\n\"\n        fmt_str += \"    # data: {}\\n\".format(self.__len__())\n        fmt_str += \"    Split: {}\\n\".format(self.split)\n        fmt_str += \"    Root: {}\".format(self.root)\n        return fmt_str\n\n\nclass BaseDataLoader(DataLoader):\n    def __init__(self, dataset, batch_size, shuffle, num_workers, val_split=0.0):\n        self.shuffle = shuffle\n        self.dataset = dataset\n        self.nbr_examples = len(dataset)\n        if val_split:\n            self.train_sampler, self.val_sampler = self._split_sampler(val_split)\n        else:\n            self.train_sampler, self.val_sampler = None, None\n\n        self.init_kwargs = {\n            'dataset': self.dataset,\n            'batch_size': batch_size,\n            'shuffle': self.shuffle,\n            'num_workers': num_workers,\n            'pin_memory': True\n        }\n        super(BaseDataLoader, self).__init__(sampler=self.train_sampler, **self.init_kwargs)\n\n    def _split_sampler(self, split):\n        if split == 0.0:\n            return None, None\n\n        self.shuffle = False\n\n        split_indx = int(self.nbr_examples * split)\n        np.random.seed(0)\n\n        indxs = np.arange(self.nbr_examples)\n        np.random.shuffle(indxs)\n        train_indxs = indxs[split_indx:]\n        val_indxs = indxs[:split_indx]\n        self.nbr_examples = len(train_indxs)\n\n        train_sampler = SubsetRandomSampler(train_indxs)\n        val_sampler = SubsetRandomSampler(val_indxs)\n        return train_sampler, val_sampler\n\n    def get_val_loader(self):\n        if self.val_sampler is None:\n            return None\n        # self.init_kwargs['batch_size'] = 1\n        return DataLoader(sampler=self.val_sampler, **self.init_kwargs)\n\n\nclass DataPrefetcher(object):\n    def __init__(self, loader, device, stop_after=None):\n        self.loader = loader\n        self.dataset = loader.dataset\n        self.stream = torch.cuda.Stream()\n        self.stop_after = stop_after\n        self.next_input = None\n        self.next_target = None\n        self.device = device\n\n    def __len__(self):\n        return len(self.loader)\n\n    def preload(self):\n        try:\n            self.next_input, self.next_target = next(self.loaditer)\n        except StopIteration:\n            self.next_input = None\n            self.next_target = None\n            return\n        with torch.cuda.stream(self.stream):\n            self.next_input = self.next_input.cuda(device=self.device, non_blocking=True)\n            self.next_target = self.next_target.cuda(device=self.device, non_blocking=True)\n\n    def __iter__(self):\n        count = 0\n        self.loaditer = iter(self.loader)\n        self.preload()\n        while self.next_input is not None:\n            torch.cuda.current_stream().wait_stream(self.stream)\n            input = self.next_input\n            target = self.next_target\n            self.preload()\n            count += 1\n            yield input, target\n            if type(self.stop_after) is int and (count > self.stop_after):\n                break\n\n\n# VOC DataLoader\n# Originally written by Kazuto Nakashima\n# https://github.com/kazuto1011/deeplab-pytorch\n\n\ndef get_voc_palette(num_classes):\n    n = num_classes\n    palette = [0] * (n * 3)\n    for j in range(0, n):\n        lab = j\n        palette[j * 3 + 0] = 0\n        palette[j * 3 + 1] = 0\n        palette[j * 3 + 2] = 0\n        i = 0\n        while (lab > 0):\n            palette[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\n            palette[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\n            palette[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\n            i = i + 1\n            lab >>= 3\n    return palette\n\n\ndef colorize_mask(mask, palette):\n    zero_pad = 256 * 3 - len(palette)\n    for i in range(zero_pad):\n        palette.append(0)\n    new_mask = PIL.Image.fromarray(mask.astype(np.uint8)).convert('P')\n    new_mask.putpalette(palette)\n    return new_mask\n\n\nclass VOCDataset(BaseDataSet):\n    \"\"\"\n    Pascal Voc dataset\n    http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        # self.num_classes = 21\n        self.num_classes = 4 + 1\n        self.palette = get_voc_palette(self.num_classes)\n        super(VOCDataset, self).__init__(**kwargs)\n\n    def _set_files(self):\n        self.root = os.path.join(self.root, 'all_aug')\n        self.image_dir = os.path.join(self.root, 'JPEGImages')\n        self.label_dir = os.path.join(self.root, 'SegmentationClass')\n\n        file_list = os.path.join(self.root, \"ImageSets/Segmentation\", self.split + \".txt\")\n        self.files = [line.rstrip() for line in tuple(open(file_list, \"r\"))]\n\n    def _load_data(self, index):\n        image_id = self.files[index]\n        image_path = os.path.join(self.image_dir, image_id + '.jpg')\n        label_path = os.path.join(self.label_dir, image_id + '.png')\n        image = np.asarray(Image.open(image_path), dtype=np.float32)\n        label = np.asarray(Image.open(label_path), dtype=np.int32)\n        print(label.min(), label.max())\n        image_id = self.files[index].split(\"/\")[-1].split(\".\")[0]\n        return image, label, image_id\n\n\nclass VOCAugDataset(BaseDataSet):\n    \"\"\"\n    Contrains both SBD and VOC 2012 dataset\n    Annotations : https://github.com/DrSleep/tensorflow-deeplab-resnet#evaluation\n    Image Sets: https://ucla.app.box.com/s/rd9z2xvwsfpksi7mi08i2xqrj7ab4keb/file/55053033642\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        # self.num_classes = 21\n        self.num_classes = 4 + 1\n        self.palette = get_voc_palette(self.num_classes)\n        super(VOCAugDataset, self).__init__(**kwargs)\n\n    def _set_files(self):\n        self.root = os.path.join(self.root, 'all_aug')\n\n        file_list = os.path.join(self.root, \"ImageSets/Segmentation\", self.split + \".txt\")\n        file_list = [line.rstrip().split(' ') for line in tuple(open(file_list, \"r\"))]\n        self.files, self.labels = list(zip(*file_list))\n\n    def _load_data(self, index):\n        image_path = os.path.join(self.root, self.files[index][1:])\n        label_path = os.path.join(self.root, self.labels[index][1:])\n        image = np.asarray(Image.open(image_path), dtype=np.float32)\n        label = np.asarray(Image.open(label_path), dtype=np.int32)\n        image_id = self.files[index].split(\"/\")[-1].split(\".\")[0]\n        return image, label, image_id\n\n\nclass VOC(BaseDataLoader):\n    def __init__(self, data_dir, batch_size, split, crop_size=None, base_size=None, scale=True, num_workers=1,\n                 val=False,\n                 shuffle=False, flip=False, rotate=False, blur=False, augment=False, val_split=None, return_id=False):\n\n        self.MEAN = [0.45734706, 0.43338275, 0.40058118]\n        self.STD = [0.23965294, 0.23532275, 0.2398498]\n\n        kwargs = {\n            'root': data_dir,\n            'split': split,\n            'mean': self.MEAN,\n            'std': self.STD,\n            'augment': augment,\n            'crop_size': crop_size,\n            'base_size': base_size,\n            'scale': scale,\n            'flip': flip,\n            'blur': blur,\n            'rotate': rotate,\n            'return_id': return_id,\n            'val': val\n        }\n\n        if split in [\"train_aug\", \"trainval_aug\", \"val_aug\", \"test_aug\"]:\n            self.dataset = VOCAugDataset(**kwargs)\n        elif split in [\"train\", \"trainval\", \"val\", \"test\"]:\n            self.dataset = VOCDataset(**kwargs)\n        else:\n            raise ValueError(f\"Invalid split name {split}\")\n        super(VOC, self).__init__(self.dataset, batch_size, shuffle, num_workers, val_split)\n\n\n# PSPNet Model\n\n\nclass BaseModel(nn.Module):\n    def __init__(self):\n        super(BaseModel, self).__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n    def forward(self):\n        raise NotImplementedError\n\n    def summary(self):\n        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n        nbr_params = sum([np.prod(p.size()) for p in model_parameters])\n        self.logger.info(f'Nbr of trainable parameters: {nbr_params}')\n\n    def __str__(self):\n        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n        nbr_params = sum([np.prod(p.size()) for p in model_parameters])\n        return super(BaseModel, self).__str__() + f'\\nNbr of trainable parameters: {nbr_params}'\n        # return summary(self, input_shape=(2, 3, 224, 224))\n\n\n# ResNet\n# Origian code and chechpoints by Hang Zhang\n# https://github.com/zhanghang1989/PyTorch-Encoding\n\n\n__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n           'resnet152', 'BasicBlock', 'Bottleneck']\n\nmodel_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n    'resnet50': 'https://hangzh.s3.amazonaws.com/encoding/models/resnet50-25c4b509.zip',\n    'resnet101': 'https://hangzh.s3.amazonaws.com/encoding/models/resnet101-2a57e44d.zip',\n    'resnet152': 'https://hangzh.s3.amazonaws.com/encoding/models/resnet152-0d43d698.zip'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    \"\"\"ResNet BasicBlock\n    \"\"\"\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None, previous_dilation=1,\n                 norm_layer=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n                               padding=dilation, dilation=dilation, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n                               padding=previous_dilation, dilation=previous_dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"ResNet Bottleneck\n    \"\"\"\n    # pylint: disable=unused-argument\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1,\n                 downsample=None, previous_dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=stride,\n            padding=dilation, dilation=dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.conv3 = nn.Conv2d(\n            planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.dilation = dilation\n        self.stride = stride\n\n    def _sum_each(self, x, y):\n        assert (len(x) == len(y))\n        z = []\n        for i in range(len(x)):\n            z.append(x[i] + y[i])\n        return z\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    \"\"\"Dilated Pre-trained ResNet Model, which preduces the stride of 8 featuremaps at conv5.\n\n    Reference:\n        - He, Kaiming, et al. \"Deep residual learning for image recognition.\" CVPR. 2016.\n        - Yu, Fisher, and Vladlen Koltun. \"Multi-scale context aggregation by dilated convolutions.\"\n    \"\"\"\n\n    # pylint: disable=unused-variable\n    def __init__(self, block, layers, num_classes=1000, dilated=True, multi_grid=False,\n                 deep_base=True, norm_layer=nn.BatchNorm2d):\n        self.inplanes = 128 if deep_base else 64\n        super(ResNet, self).__init__()\n        if deep_base:\n            self.conv1 = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False),\n                norm_layer(64),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n                norm_layer(64),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n            )\n        else:\n            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                                   bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        if dilated:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n                                           dilation=2, norm_layer=norm_layer)\n            if multi_grid:\n                self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                               dilation=4, norm_layer=norm_layer,\n                                               multi_grid=True)\n            else:\n                self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                               dilation=4, norm_layer=norm_layer)\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                           norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                           norm_layer=norm_layer)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, norm_layer):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None, multi_grid=False):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        multi_dilations = [4, 8, 16]\n        if multi_grid:\n            layers.append(block(self.inplanes, planes, stride, dilation=multi_dilations[0],\n                                downsample=downsample, previous_dilation=dilation, norm_layer=norm_layer))\n        elif dilation == 1 or dilation == 2:\n            layers.append(block(self.inplanes, planes, stride, dilation=1,\n                                downsample=downsample, previous_dilation=dilation, norm_layer=norm_layer))\n        elif dilation == 4:\n            layers.append(block(self.inplanes, planes, stride, dilation=2,\n                                downsample=downsample, previous_dilation=dilation, norm_layer=norm_layer))\n        else:\n            raise RuntimeError(\"=> unknown dilation size: {}\".format(dilation))\n\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            if multi_grid:\n                layers.append(block(self.inplanes, planes, dilation=multi_dilations[i],\n                                    previous_dilation=dilation, norm_layer=norm_layer))\n            else:\n                layers.append(block(self.inplanes, planes, dilation=dilation, previous_dilation=dilation,\n                                    norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    \"\"\"Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    \"\"\"Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n    return model\n\n\ndef resnet50(pretrained=False, root='./pretrained', **kwargs):\n    \"\"\"Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls['resnet50'], model_dir=root))\n    return model\n\n\ndef resnet101(pretrained=False, root='./pretrained', **kwargs):\n    \"\"\"Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls['resnet101'], model_dir=root))\n    return model\n\n\ndef resnet152(pretrained=False, root='./pretrained', **kwargs):\n    \"\"\"Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls['resnet152'], model_dir=root))\n    return model\n\n\ndef load_url(url, model_dir='./pretrained', map_location=None):\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    filename = url.split('/')[-1].split('.')[0]\n    cached_file = os.path.join(model_dir, filename + '.pth')\n    if not os.path.exists(cached_file):\n        cached_file = os.path.join(model_dir, filename + '.zip')\n        sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\n        urlretrieve(url, cached_file)\n        zip_ref = zipfile.ZipFile(cached_file, 'r')\n        zip_ref.extractall(model_dir)\n        zip_ref.close()\n        os.remove(cached_file)\n        cached_file = os.path.join(model_dir, filename + '.pth')\n    return torch.load(cached_file, map_location=map_location)\n\n\ndef initialize_weights(*models):\n    for model in models:\n        for m in model.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight.data, nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1.)\n                m.bias.data.fill_(1e-4)\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0.0, 0.0001)\n                m.bias.data.zero_()\n\n\ndef set_trainable_attr(m, b):\n    m.trainable = b\n    for p in m.parameters(): p.requires_grad = b\n\n\ndef apply_leaf(m, f):\n    c = m if isinstance(m, (list, tuple)) else list(m.children())\n    if isinstance(m, nn.Module):\n        f(m)\n    if len(c) > 0:\n        for l in c:\n            apply_leaf(l, f)\n\n\ndef set_trainable(l, b):\n    apply_leaf(l, lambda m: set_trainable_attr(m, b))\n\n\nclass _PSPModule(nn.Module):\n    def __init__(self, in_channels, bin_sizes, norm_layer):\n        super(_PSPModule, self).__init__()\n        out_channels = in_channels // len(bin_sizes)\n        self.stages = nn.ModuleList([self._make_stages(in_channels, out_channels, b_s, norm_layer)\n                                     for b_s in bin_sizes])\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(in_channels + (out_channels * len(bin_sizes)), out_channels,\n                      kernel_size=3, padding=1, bias=False),\n            norm_layer(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1)\n        )\n\n    def _make_stages(self, in_channels, out_channels, bin_sz, norm_layer):\n        prior = nn.AdaptiveAvgPool2d(output_size=bin_sz)\n        conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        bn = norm_layer(out_channels)\n        relu = nn.ReLU(inplace=True)\n        return nn.Sequential(prior, conv, bn, relu)\n\n    def forward(self, features):\n        h, w = features.size()[2], features.size()[3]\n        pyramids = [features]\n        pyramids.extend([F.interpolate(stage(features), size=(h, w), mode='bilinear',\n                                       align_corners=True) for stage in self.stages])\n        output = self.bottleneck(torch.cat(pyramids, dim=1))\n        return output\n\n\nclass PSPNet(BaseModel):\n    def __init__(self, num_classes, in_channels=3, backbone='resnet152', pretrained=False, use_aux=True, freeze_bn=False,\n                 freeze_backbone=False):\n        super(PSPNet, self).__init__()\n        # TODO: Use synch batchnorm\n        norm_layer = nn.BatchNorm2d\n        # model = getattr(resnet, backbone)(pretrained, norm_layer=norm_layer, )\n        model = resnet50(pretrained, norm_layer=norm_layer, )\n        m_out_sz = model.fc.in_features\n        self.use_aux = use_aux\n\n        self.initial = nn.Sequential(*list(model.children())[:4])\n        if in_channels != 3:\n            self.initial[0] = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.initial = nn.Sequential(*self.initial)\n\n        self.layer1 = model.layer1\n        self.layer2 = model.layer2\n        self.layer3 = model.layer3\n        self.layer4 = model.layer4\n\n        self.master_branch = nn.Sequential(\n            _PSPModule(m_out_sz, bin_sizes=[1, 2, 3, 6], norm_layer=norm_layer),\n            nn.Conv2d(m_out_sz // 4, num_classes, kernel_size=1)\n        )\n\n        self.auxiliary_branch = nn.Sequential(\n            nn.Conv2d(m_out_sz // 2, m_out_sz // 4, kernel_size=3, padding=1, bias=False),\n            norm_layer(m_out_sz // 4),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1),\n            nn.Conv2d(m_out_sz // 4, num_classes, kernel_size=1)\n        )\n\n        initialize_weights(self.master_branch, self.auxiliary_branch)\n        if freeze_bn: self.freeze_bn()\n        if freeze_backbone:\n            set_trainable([self.initial, self.layer1, self.layer2, self.layer3, self.layer4], False)\n\n    def forward(self, x):\n        input_size = (x.size()[2], x.size()[3])\n        x = self.initial(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x_aux = self.layer3(x)\n        x = self.layer4(x_aux)\n\n        output = self.master_branch(x)\n        output = F.interpolate(output, size=input_size, mode='bilinear')\n        output = output[:, :, :input_size[0], :input_size[1]]\n\n        if self.training and self.use_aux:\n            aux = self.auxiliary_branch(x_aux)\n            aux = F.interpolate(aux, size=input_size, mode='bilinear')\n            aux = aux[:, :, :input_size[0], :input_size[1]]\n            return output, aux\n        return output\n\n    def get_backbone_params(self):\n        return chain(self.initial.parameters(), self.layer1.parameters(), self.layer2.parameters(),\n                     self.layer3.parameters(), self.layer4.parameters())\n\n    def get_decoder_params(self):\n        return chain(self.master_branch.parameters(), self.auxiliary_branch.parameters())\n\n    def freeze_bn(self):\n        for module in self.modules():\n            if isinstance(module, nn.BatchNorm2d): module.eval()\n\n\n## PSP with dense net as the backbone\n\nclass PSPDenseNet(BaseModel):\n    def __init__(self, num_classes, in_channels=3, backbone='densenet201', pretrained=True, use_aux=True,\n                 freeze_bn=False, **_):\n        super(PSPDenseNet, self).__init__()\n        self.use_aux = use_aux\n        model = getattr(models, backbone)(pretrained)\n        m_out_sz = model.classifier.in_features\n        aux_out_sz = model.features.transition3.conv.out_channels\n\n        if not pretrained or in_channels != 3:\n            # If we're training from scratch, better to use 3x3 convs\n            block0 = [nn.Conv2d(in_channels, 64, 3, stride=2, bias=False), nn.BatchNorm2d(64), nn.ReLU(inplace=True)]\n            block0.extend(\n                [nn.Conv2d(64, 64, 3, bias=False), nn.BatchNorm2d(64), nn.ReLU(inplace=True)] * 2\n            )\n            self.block0 = nn.Sequential(\n                *block0,\n                nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n            )\n            initialize_weights(self.block0)\n        else:\n            self.block0 = nn.Sequential(*list(model.features.children())[:4])\n\n        self.block1 = model.features.denseblock1\n        self.block2 = model.features.denseblock2\n        self.block3 = model.features.denseblock3\n        self.block4 = model.features.denseblock4\n\n        self.transition1 = model.features.transition1\n        # No pooling\n        self.transition2 = nn.Sequential(\n            *list(model.features.transition2.children())[:-1])\n        self.transition3 = nn.Sequential(\n            *list(model.features.transition3.children())[:-1])\n\n        for n, m in self.block3.named_modules():\n            if 'conv2' in n:\n                m.dilation, m.padding = (2, 2), (2, 2)\n        for n, m in self.block4.named_modules():\n            if 'conv2' in n:\n                m.dilation, m.padding = (4, 4), (4, 4)\n\n        self.master_branch = nn.Sequential(\n            _PSPModule(m_out_sz, bin_sizes=[1, 2, 3, 6], norm_layer=nn.BatchNorm2d),\n            nn.Conv2d(m_out_sz // 4, num_classes, kernel_size=1)\n        )\n\n        self.auxiliary_branch = nn.Sequential(\n            nn.Conv2d(aux_out_sz, m_out_sz // 4, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(m_out_sz // 4),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1),\n            nn.Conv2d(m_out_sz // 4, num_classes, kernel_size=1)\n        )\n\n        initialize_weights(self.master_branch, self.auxiliary_branch)\n        if freeze_bn: self.freeze_bn()\n\n    def forward(self, x):\n        input_size = (x.size()[2], x.size()[3])\n\n        x = self.block0(x)\n        x = self.block1(x)\n        x = self.transition1(x)\n        x = self.block2(x)\n        x = self.transition2(x)\n        x = self.block3(x)\n        x_aux = self.transition3(x)\n        x = self.block4(x_aux)\n\n        output = self.master_branch(x)\n        output = F.interpolate(output, size=input_size, mode='bilinear')\n\n        if self.training and self.use_aux:\n            aux = self.auxiliary_branch(x_aux)\n            aux = F.interpolate(aux, size=input_size, mode='bilinear')\n            return output, aux\n        return output\n\n    def get_backbone_params(self):\n        return chain(self.block0.parameters(), self.block1.parameters(), self.block2.parameters(),\n                     self.block3.parameters(), self.transition1.parameters(), self.transition2.parameters(),\n                     self.transition3.parameters())\n\n    def get_decoder_params(self):\n        return chain(self.master_branch.parameters(), self.auxiliary_branch.parameters())\n\n    def freeze_bn(self):\n        for module in self.modules():\n            if isinstance(module, nn.BatchNorm2d): module.eval()\n\n\n# Inference\ndef pad_image(img, target_size):\n    rows_to_pad = max(target_size[0] - img.shape[2], 0)\n    cols_to_pad = max(target_size[1] - img.shape[3], 0)\n    padded_img = F.pad(img, (0, cols_to_pad, 0, rows_to_pad), \"constant\", 0)\n    return padded_img\n\n\ndef sliding_predict(model, image, num_classes, flip=True):\n    image_size = image.shape\n    tile_size = (int(image_size[2] // 2.5), int(image_size[3] // 2.5))\n    overlap = 1 / 3\n\n    stride = ceil(tile_size[0] * (1 - overlap))\n\n    num_rows = int(ceil((image_size[2] - tile_size[0]) / stride) + 1)\n    num_cols = int(ceil((image_size[3] - tile_size[1]) / stride) + 1)\n    total_predictions = np.zeros((num_classes, image_size[2], image_size[3]))\n    count_predictions = np.zeros((image_size[2], image_size[3]))\n    tile_counter = 0\n\n    for row in range(num_rows):\n        for col in range(num_cols):\n            x_min, y_min = int(col * stride), int(row * stride)\n            x_max = min(x_min + tile_size[1], image_size[3])\n            y_max = min(y_min + tile_size[0], image_size[2])\n\n            img = image[:, :, y_min:y_max, x_min:x_max]\n            padded_img = pad_image(img, tile_size)\n            tile_counter += 1\n            padded_prediction = model(padded_img)\n            if flip:\n                fliped_img = padded_img.flip(-1)\n                fliped_predictions = model(padded_img.flip(-1))\n                padded_prediction = 0.5 * (fliped_predictions.flip(-1) + padded_prediction)\n            predictions = padded_prediction[:, :, :img.shape[2], :img.shape[3]]\n            count_predictions[y_min:y_max, x_min:x_max] += 1\n            total_predictions[:, y_min:y_max, x_min:x_max] += predictions.data.cpu().numpy().squeeze(0)\n\n    total_predictions /= count_predictions\n    return total_predictions\n\n\ndef multi_scale_predict(model, image, scales, num_classes, device, flip=False):\n    input_size = (image.size(2), image.size(3))\n    upsample = nn.Upsample(size=input_size, mode='bilinear', align_corners=True)\n    total_predictions = np.zeros((num_classes, image.size(2), image.size(3)))\n\n    image = image.data.data.cpu().numpy()\n    for scale in scales:\n        scaled_img = ndimage.zoom(image, (1.0, 1.0, float(scale), float(scale)), order=1, prefilter=False)\n        scaled_img = torch.from_numpy(scaled_img).to(device)\n        scaled_prediction = upsample(model(scaled_img).cpu())\n\n        if flip:\n            fliped_img = scaled_img.flip(-1).to(device)\n            fliped_predictions = upsample(model(fliped_img).cpu())\n            scaled_prediction = 0.5 * (fliped_predictions.flip(-1) + scaled_prediction)\n        total_predictions += scaled_prediction.data.cpu().numpy().squeeze(0)\n\n    total_predictions /= len(scales)\n    return total_predictions\n\n\ndef save_images(image, mask, output_path, image_file, palette):\n    # Saves the image, the model output and the results after the post processing\n    w, h = image.size\n    np.set_printoptions(threshold=sys.maxsize)\n    image_file = os.path.basename(image_file).split('.')[0]\n    print('mask.shape', mask.shape)\n    print('mask', mask)\n    colorized_mask = colorize_mask(mask, palette)\n    colorized_mask.save(os.path.join(output_path, image_file + '.png'))\n    # output_im = Image.new('RGB', (w*2, h))\n    # output_im.paste(image, (0,0))\n    # output_im.paste(colorized_mask, (w,0))\n    # output_im.save(os.path.join(output_path, image_file+'_colorized.png'))\n    # mask_img = Image.fromarray(mask, 'L')\n    # mask_img.save(os.path.join(output_path, image_file+'.png'))\n\n\ndef rle_encode_2(img):\n    '''\n    #img: numpy array, 1 - mask, 0 - background\n    #Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n\ndef decode_per_class_rle(label_mask):\n    # np.set_printoptions(threshold=sys.maxsize)\n    \"\"\"Decode segmentation class labels into a seprate numpy arrays of masks containing segmentation area for each class\n       in seprate mask\n    Args:\n        label_mask (np.ndarray): an (M,N) array of integer values denoting\n          the class label at each spatial location.\n        plot (bool, optional): whether to show the resulting color image\n          in a figure.\n    Returns:\n        (list of np array of masks per class, Run Length Encodings (rle) list per class).\n    \"\"\"\n    outputs = []\n    n_classes = 4+1\n    classes_data = []\n    classes_rle = []\n    ### label_mask = np.transpose(label_mask)\n    ### print(\"label_maskiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\", type(label_mask))\n    for cls in range(1, n_classes):\n        label_mask_i = label_mask\n        label_mask_i = (label_mask_i == cls)\n        label_mask_i = label_mask_i.astype(int)\n        rle2 = rle_encode_2(label_mask_i)\n        outputs.append(rle2)\n    return outputs\n\ndef main():\n    model_config_path = '../input/configk1/configkeg.json'\n    #ckpt = '../input/modelv1/checkpoint-epoch10.pth'\n    ckpt = '../input/modelv4/trimedmodelv3.pth'\n    testimgs_path = '../input/severstal-steel-defect-detection/test_images/'\n    extension = 'jpg'\n    csv_save_path = 'submission.csv'\n    sample_sub_path = '../input/severstal-steel-defect-detection/'\n    #mode = 'multiscale'\n    mode = None\n\n    #sample_sub = pd.read_csv(sample_sub_path+'sample-submission.csv')\n    sample_sub = pd.read_csv('../input/severstal-steel-defect-detection/sample_submission.csv')\n    sample_sub['ImageId'] = sample_sub['ImageId_ClassId'].map(lambda x: x.split('_')[0])\n\n    config = json.load(open(model_config_path))\n\n    # Dataset used for training the model\n    dataset_type = config['train_loader']['type']\n    assert dataset_type in ['VOC', 'COCO', 'CityScapes', 'ADE20K']\n    if dataset_type == 'CityScapes':\n        scales = [0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.25]\n    else:\n        scales = [0.75, 1.0, 1.25, 1.5, 1.75, 2.0]\n    # loader = getattr(dataloaders, config['train_loader']['type'])(**config['train_loader']['args'])\n    # loader = VOC(**config['train_loader']['args'])\n    to_tensor = transforms.ToTensor()\n    meani = [0.45734706, 0.43338275, 0.40058118]\n    stdi = [0.23965294, 0.23532275, 0.2398498]\n    normalize = transforms.Normalize(meani, stdi)\n    num_classes = 4+1\n    # palette = loader.dataset.palette\n\n    # Model\n    ### model = getattr(models, config['arch']['type'])(num_classes, **config['arch']['args'])\n    model = PSPNet(num_classes, **config['arch']['args'])\n    availble_gpus = list(range(torch.cuda.device_count()))\n    device = torch.device('cuda:0' if len(availble_gpus) > 0 else 'cpu')\n    #Important: Load only 'state_dict' model; no other key!\n    checkpoint = torch.load(ckpt)\n    #checkpoint = torch.load(ckpt, map_location='cpu')\n    if isinstance(checkpoint, dict) and 'state_dict' in checkpoint.keys():\n        checkpoint = checkpoint['state_dict']\n    if 'module' in list(checkpoint.keys())[0] and not isinstance(model, torch.nn.DataParallel):\n        model = torch.nn.DataParallel(model)\n\n    model.load_state_dict(checkpoint)\n    model.to(device)\n    model.eval()\n\n    if not os.path.exists('outputs'):\n        os.makedirs('outputs')\n    all_outs = []\n    image_files = sorted(glob(os.path.join(testimgs_path, f'*.{extension}')))\n    with torch.no_grad():\n        tbar = tqdm(image_files, ncols=100)\n        for img_file in tbar:\n            image = Image.open(img_file).convert('RGB')\n            input = normalize(to_tensor(image)).unsqueeze(0)\n            if mode == 'multiscale':\n                prediction = multi_scale_predict(model, input, scales, num_classes, device)\n            elif mode == 'sliding':\n                prediction = sliding_predict(model, input, num_classes)\n            else:\n                prediction = model(input.to(device))\n                prediction = prediction.squeeze(0).cpu().numpy()\n            prediction = F.softmax(torch.from_numpy(prediction), dim=0).argmax(0).cpu().numpy()\n            # save_images(image, prediction, args.output, img_file, palette)\n            outs = decode_per_class_rle(prediction)\n            all_outs.extend(outs)\n        assert len(sample_sub) == len(all_outs)\n        sample_sub['EncodedPixels'] = all_outs\n        sample_sub.drop(columns='ImageId').to_csv(csv_save_path, index=False)\n\n\nif __name__ == '__main__':\n    main()","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}